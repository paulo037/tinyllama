{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional,  Tuple\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO refactor the code\n",
    "# TODO code load dataset and dataloader\n",
    "# TODO code training loop\n",
    "# TODO code evaluation loop\n",
    "# TODO add support to tensorboard\n",
    "# TODO add weights initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    name: str = \"tiny_LLaMA_1b\"\n",
    "\n",
    "    seq_length: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    \n",
    "    n_layer: int = 22\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 2048\n",
    "    hidden_dim: int = 5632\n",
    "    n_query_groups: int = 4\n",
    "\n",
    "    base=10000\n",
    "    rotary_percentage: float = 1.0\n",
    "    \n",
    "    device: str = \"cpu\"\n",
    "    dtype=torch.float16\n",
    "    \n",
    "    stop_token_id = 2\n",
    "\n",
    "    @property\n",
    "    def head_size(self) -> int:\n",
    "        return self.n_embd // self.n_head\n",
    "      \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return int(self.rotary_percentage * self.head_size)\n",
    "\n",
    "configs = [\n",
    "    dict(\n",
    "        name=\"tiny_LLaMA_1b\",\n",
    "        seq_length=2048,\n",
    "        vocab_size=32000,\n",
    "        n_layer=22,\n",
    "        n_head=32,\n",
    "        n_embd=2048,\n",
    "        rotary_percentage=1.0,\n",
    "        hidden_dim=5632,\n",
    "        n_query_groups=4,\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"tiny_tiny_LLaMA\",\n",
    "        seq_length=256,\n",
    "        vocab_size=32000,\n",
    "        n_layer=12,\n",
    "        n_head=8,\n",
    "        n_query_groups=4,\n",
    "        n_embd=768,\n",
    "        rotary_percentage=1.0,\n",
    "        hidden_dim=512,\n",
    "    )\n",
    "]\n",
    "\n",
    "name_to_config: dict[str, Config] = {\n",
    "    config[\"name\"]: Config(**config) for config in configs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoPe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "      \n",
    "\n",
    "        self.config = config\n",
    "        inv_freq, self.attention_scaling = self._compute_rope_parameters(self.config)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "        \n",
    "    def _compute_rope_parameters(\n",
    "        self,\n",
    "        config: Config  \n",
    "    ) -> Tuple[torch.Tensor, float]:\n",
    "\n",
    "        base = config.base\n",
    "        dim = config.dim\n",
    "        attention_factor = 1.0  \n",
    "        \n",
    "        # Compute the inverse frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(config.device) / dim))\n",
    "        return inv_freq, attention_factor\n",
    "      \n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "  \n",
    "def apply_rotary_pos_emb(q, k, cos, sin,  unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupQueryAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = config.dtype\n",
    "        self.dim = config.n_embd\n",
    "        self.query_heads = config.n_head\n",
    "        self.queries_per_kv = config.n_head // config.n_query_groups\n",
    "        self.key_value_heads = config.n_head // self.queries_per_kv\n",
    "\n",
    "        self.kv_dim = self.dim // self.query_heads * self.key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.dim, self.dim,\n",
    "                                bias=False, dtype=self.dtype)\n",
    "        self.k_proj = nn.Linear(self.dim, self.kv_dim,\n",
    "                                bias=False, dtype=self.dtype)\n",
    "        self.v_proj = nn.Linear(self.dim, self.kv_dim,\n",
    "                                bias=False, dtype=self.dtype)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.dim, self.dim,\n",
    "                                bias=False, dtype=self.dtype)\n",
    "        self.config = config\n",
    "\n",
    "    def scaled_dot_product_gqa(self, query: Tensor, key: Tensor, value: Tensor):\n",
    "        scale_factor = 1 / query.size(-1) ** 0.5\n",
    "\n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "\n",
    "        attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "        query = rearrange(query, \"b (h g) n d -> b g h n d\",\n",
    "                          g=self.queries_per_kv)\n",
    "\n",
    "        attn_weight = query @ key.transpose(-1, -2) * scale_factor\n",
    "\n",
    "        attn_weight += attn_bias\n",
    "\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        y = attn_weight @ value\n",
    "        y = rearrange(y, \"b g h n d -> b (h g) n d\")\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, \n",
    "                x: Tensor,\n",
    "                position_embeddings: Tensor) -> torch.Tensor:\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.query_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.key_value_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.key_value_heads)\n",
    "\n",
    "            \n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        y = self.scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v\n",
    "        )\n",
    "\n",
    "        y = rearrange(y, \"b h n d -> b n (h d)\")\n",
    "\n",
    "        y = self.o_proj(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.gate_proj = nn.Linear(config.n_embd, config.hidden_dim, bias=False, dtype=config.dtype)\n",
    "        self.up_proj = nn.Linear(config.n_embd, config.hidden_dim, bias=False, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.n_embd, bias=False, dtype=config.dtype)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config: Config,  dim: int = -1, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(config.n_embd, dtype=config.dtype))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.attn = GroupQueryAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "\n",
    "    def forward(self, x, pos_emb, mask=None):\n",
    "        n_1 = self.norm1(x)\n",
    "        h = self.attn(n_1, pos_emb)\n",
    "        x = x + h\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd , dtype=config.dtype)\n",
    "        self.layers       = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layer)])\n",
    "        self.norm         = RMSNorm(config)\n",
    "        self.lm_head      = nn.Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "        \n",
    "        self.rope = LlamaRotaryEmbedding(config)\n",
    "        self.rope_cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "\n",
    "    def get_kv_head_dim(self, config: Config) -> int:\n",
    "        queries_per_kv = config.n_head // config.n_query_groups\n",
    "        key_value_heads = config.n_head // queries_per_kv\n",
    "        kv_dim = config.hidden_dim // config.n_head * key_value_heads\n",
    "        return kv_dim\n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        position_ids = torch.arange(self.config.seq_length, device=idx.device).unsqueeze(0)\n",
    "        return self.rope(idx, position_ids)\n",
    "      \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "    ) -> Tensor:\n",
    "      \n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        assert T <= self.config.seq_length, f\"Input length {T} exceeds maximum model length {self.config.seq_length}\"\n",
    "        \n",
    "        max_seq_length = self.config.seq_length\n",
    "        x = self.embed_tokens(input_ids)  \n",
    "        \n",
    "        if self.rope_cache is None :\n",
    "            self.rope_cache = self.build_rope_cache(x)\n",
    "        \n",
    "        cos, sin = self.rope_cache\n",
    "        \n",
    "        cos = cos[:, :T]\n",
    "        sin = sin[:, :T]\n",
    "\n",
    "        for block in self.layers:\n",
    "            x = block(x, (cos, sin), max_seq_length)\n",
    "\n",
    "        x = self.norm(x)\n",
    "    \n",
    "        return self.lm_head(x) \n",
    "\n",
    "    def generate(self, input_ids: torch.LongTensor, max_length: int = Optional[100], sample: Optional[bool] = False) -> torch.LongTensor:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                logits = self(input_ids)\n",
    "                if sample:\n",
    "                    next_token = torch.multinomial(logits[:, -1].softmax(dim=-1), num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits[:, -1], dim=-1).unsqueeze(-1)\n",
    "                    \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "                if next_token.item() == self.config.stop_token_id:\n",
    "                    break\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remap_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Remaps the state dict keys from LlamaForCausalLM format to your custom model format\n",
    "    \"\"\"\n",
    "    new_state_dict = OrderedDict()\n",
    "    \n",
    "    # Create a mapping dictionary for the different naming conventions\n",
    "    key_mapping = {\n",
    "        'model.embed_tokens': 'embed_tokens',\n",
    "        'model.norm': 'norm',\n",
    "        'model.layers': 'layers',\n",
    "        'self_attn': 'attn',\n",
    "        'input_layernorm': 'norm1',\n",
    "        'post_attention_layernorm': 'norm2',\n",
    "        'mlp': 'ffn'\n",
    "    }\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key\n",
    "        \n",
    "        # Apply the mappings\n",
    "        for old, new in key_mapping.items():\n",
    "            new_key = new_key.replace(old, new)\n",
    "            \n",
    "        # Handle specific cases where tensor shapes need to be validated\n",
    "        if 'attn' in new_key:\n",
    "            # Ensure attention weights have compatible shapes\n",
    "            if value.shape != state_dict[key].shape:\n",
    "                raise ValueError(f\"Incompatible shape for attention weights: {key}\")\n",
    "                \n",
    "        new_state_dict[new_key] = value\n",
    "    \n",
    "    return new_state_dict\n",
    "\n",
    "def load_model_weights(model, safetensors_path):\n",
    "    \"\"\"\n",
    "    Loads and remaps weights from a safetensors file to your custom model\n",
    "    \n",
    "    Args:\n",
    "        model: Your custom model instance\n",
    "        safetensors_path: Path to the safetensors file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the safetensors file\n",
    "        original_state_dict = load_file(safetensors_path)\n",
    "        \n",
    "        # Remap the state dict to match your model's architecture\n",
    "        new_state_dict = remap_state_dict(original_state_dict)\n",
    "        \n",
    "        # Load the remapped weights into your model\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        if missing_keys:\n",
    "            print(\"Missing keys:\", missing_keys)\n",
    "        if unexpected_keys:\n",
    "            print(\"Unexpected keys:\", unexpected_keys)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_LLaMA_1b = LlamaModel(name_to_config[\"tiny_LLaMA_1b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_weights(tiny_LLaMA_1b, \"model/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,  map_device=\"auto\", add_eos_token=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"model\",\n",
    "    # attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "out1 = tiny_LLaMA_1b(inputs_ids)\n",
    "out2 = model(inputs_ids, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (out1 == out2.logits).all(), \"The model output is different from the reference model output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {'role': 'user', 'content': 'Hello, how are you?'},\n",
    "]\n",
    "\n",
    "inputs_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tiny_LLaMA_1b.generate(inputs_ids, max_length=10, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHello, how are you?</s> \\n<|assistant|>\\nI am doing well, thank you. How are'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
