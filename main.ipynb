{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/.pyenv/versions/3.11.3/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/paulo/.pyenv/versions/3.11.3/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/home/paulo/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Literal, Optional, Type, Tuple, List\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, einsum\n",
    "from xformers.ops import SwiGLU\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix Rotatory Embeddings\n",
    "# TODO fix flashAttention\n",
    "# TODO remove unused config parameters\n",
    "# TODO refactor the code\n",
    "# TODO fix cache\n",
    "# TODO code load dataset and dataloader\n",
    "# TODO code training loop\n",
    "# TODO code evaluation loop\n",
    "# TODO code inference loop\n",
    "# TODO add support to tensorboard\n",
    "# TODO add weights initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    org: str = \"StatNLP-research\"\n",
    "    name: str = \"tiny_LLaMA_1b\"\n",
    "    \n",
    "    seq_length: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padding_multiple: int = 64\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    \n",
    "    n_layer: int = 22\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 2048\n",
    "\n",
    "    rotary_percentage: float = 1.0\n",
    "    parallel_residual: bool = False\n",
    "    bias: bool = False\n",
    "    n_query_groups: int = 4\n",
    "    shared_attention_norm: bool = False\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    hidden_dim: int = 5632\n",
    "    condense_ratio: int = 1\n",
    "    dropout: int = 0.0\n",
    "    device: str = \"cpu\"\n",
    "    dtype=torch.float16\n",
    "\n",
    "    @property\n",
    "    def head_size(self) -> int:\n",
    "        return self.n_embd // self.n_head\n",
    "\n",
    "configs = [\n",
    "    dict(\n",
    "        org=\"StatNLP-research\",\n",
    "        name=\"tiny_LLaMA_1b\",\n",
    "        seq_length=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=22,\n",
    "        n_head=32,\n",
    "        n_embd=2048,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_eps=1e-5,\n",
    "        hidden_dim=5632,\n",
    "        n_query_groups=4,\n",
    "    ),\n",
    "    dict(\n",
    "        org=\"ufv\",\n",
    "        name=\"tiny_tiny_LLaMA\",\n",
    "        seq_length=256,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=12,\n",
    "        n_head=8,\n",
    "        n_query_groups=4,\n",
    "        n_embd=768,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_eps=1e-5,\n",
    "        hidden_dim=512,\n",
    "    )\n",
    "]\n",
    "\n",
    "name_to_config: dict[str, Config] = {\n",
    "    config[\"name\"]: Config(**config) for config in configs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoPe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LlamaRotaryEmbedding(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dim=None,\n",
    "#         max_position_embeddings=2048,\n",
    "#         base=10000,\n",
    "#         device=None,\n",
    "#         scaling_factor=1.0,\n",
    "#         rope_type=\"default\",\n",
    "#         config: Optional[LlamaConfig] = None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # TODO (joao): remove the `if` below, only used for BC\n",
    "#         self.rope_kwargs = {}\n",
    "#         if config is None:\n",
    "#             logger.warning_once(\n",
    "#                 \"`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n",
    "#                 \"`config` argument. All other arguments will be removed in v4.46\"\n",
    "#             )\n",
    "#             self.rope_kwargs = {\n",
    "#                 \"rope_type\": rope_type,\n",
    "#                 \"factor\": scaling_factor,\n",
    "#                 \"dim\": dim,\n",
    "#                 \"base\": base,\n",
    "#                 \"max_position_embeddings\": max_position_embeddings,\n",
    "#             }\n",
    "#             self.rope_type = rope_type\n",
    "#             self.max_seq_len_cached = max_position_embeddings\n",
    "#             self.original_max_seq_len = max_position_embeddings\n",
    "#         else:\n",
    "#             # BC: \"rope_type\" was originally \"type\"\n",
    "#             if config.rope_scaling is not None:\n",
    "#                 self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "#             else:\n",
    "#                 self.rope_type = \"default\"\n",
    "#             self.max_seq_len_cached = config.max_position_embeddings\n",
    "#             self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "#         self.config = config\n",
    "#         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "#         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "#         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "#         self.original_inv_freq = self.inv_freq\n",
    "\n",
    "#     def _dynamic_frequency_update(self, position_ids, device):\n",
    "#             self.max_seq_len_cached = self.original_max_seq_len\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, x, position_ids):\n",
    "#         if \"dynamic\" in self.rope_type:\n",
    "#             self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "#         # Core RoPE block\n",
    "#         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "#         position_ids_expanded = position_ids[:, None, :].float()\n",
    "#         # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "#         device_type = x.device.type\n",
    "#         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "#         with torch.autocast(device_type=device_type, enabled=False):\n",
    "#             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "#             emb = torch.cat((freqs, freqs), dim=-1)\n",
    "#             cos = emb.cos()\n",
    "#             sin = emb.sin()\n",
    "\n",
    "#         # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "#         cos = cos * self.attention_scaling\n",
    "#         sin = sin * self.attention_scaling\n",
    "\n",
    "#         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000, condense_ratio: int = 1\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Build rotary position embedding cache.\n",
    "    Note: n_elem should be head_size, not head_size//2\n",
    "    \"\"\"\n",
    "\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device) / n_elem))\n",
    "\n",
    "    seq_idx = torch.arange(seq_len, device=device) / condense_ratio\n",
    "    \n",
    "    idx_theta = torch.outer(seq_idx, theta)\n",
    "\n",
    "    # Expand dimensions to match broadcasting needs\n",
    "    cos = torch.cos(idx_theta).unsqueeze(1)  \n",
    "    sin = torch.sin(idx_theta).unsqueeze(1)  \n",
    "    \n",
    "    # Duplicate each value to match full head size\n",
    "    cos = torch.repeat_interleave(cos, 2, dim=-1)  \n",
    "    sin = torch.repeat_interleave(sin, 2, dim=-1)  \n",
    "    \n",
    "    if dtype == torch.bfloat16:\n",
    "        return cos.bfloat16(), sin.bfloat16()\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        return cos.half(), sin.half()\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "      Reference implementation: https://github.com/jzhang38/TinyLlama/blob/main/lit_gpt/model.py\n",
    "    \"\"\"\n",
    "    head_size = x.size(-1)\n",
    "    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
    "    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "  \n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupQueryAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = config.dtype\n",
    "        self.dim = config.n_embd\n",
    "        self.query_heads = config.n_head \n",
    "        self.queries_per_kv = config.n_head // config.n_query_groups\n",
    "        self.key_value_heads = config.n_head // self.queries_per_kv\n",
    "\n",
    "        self.kv_dim = self.dim // self.query_heads * self.key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.dim, self.dim,    bias=False, dtype=self.dtype)\n",
    "        self.k_proj = nn.Linear(self.dim, self.kv_dim, bias=False, dtype=self.dtype)\n",
    "        self.v_proj = nn.Linear(self.dim, self.kv_dim, bias=False, dtype=self.dtype)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.dim, self.dim,    bias=False, dtype=self.dtype)\n",
    "        self.config = config\n",
    "        \n",
    "    \n",
    "    def scaled_dot_product_gqa(self, query: Tensor, key:Tensor, value:Tensor):\n",
    "        scale_factor = 1 / query.size(-1) ** 0.5 \n",
    "        \n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "\n",
    "        attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "        query = rearrange(query, \"b (h g) n d -> b g h n d\", g=self.queries_per_kv)\n",
    "\n",
    "        attn_weight = query @ key.transpose(-1,-2) * scale_factor\n",
    "\n",
    "        attn_weight += attn_bias\n",
    "        \n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        y = attn_weight @ value\n",
    "        y = rearrange(y, \"b g h n d -> b (h g) n d\")\n",
    "\n",
    "        return y\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor, position_embeddings: Tensor,  mask: Optional[Tensor] = None) -> torch.Tensor:\n",
    "\n",
    "        \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.query_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.key_value_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.key_value_heads)\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "        \n",
    "\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        \n",
    "        y = self.scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v\n",
    "        )\n",
    "        \n",
    "        y = rearrange(y, \"b h n d -> b n (h d)\")\n",
    "\n",
    "        y = self.o_proj(y)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.gate_proj = nn.Linear(config.n_embd, config.hidden_dim, bias=False, dtype=config.dtype)\n",
    "        self.up_proj = nn.Linear(config.n_embd, config.hidden_dim, bias=False, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(config.hidden_dim, config.n_embd, bias=False, dtype=config.dtype)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config: Config,  dim: int = -1, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(config.n_embd, dtype=config.dtype))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.attn = GroupQueryAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "\n",
    "    def forward(self, x, pos_emb, mask=None):\n",
    "        n_1 = self.norm1(x)\n",
    "        h = self.attn(n_1, pos_emb, mask=mask)\n",
    "        x = x + h\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd , dtype=config.dtype)\n",
    "        self.layers       = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layer)])\n",
    "        self.norm         = RMSNorm(config)\n",
    "        self.lm_head      = nn.Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "        \n",
    "        self.rope = LlamaRotaryEmbedding(max_position_embeddings=config.seq_length, dim=int(config.rotary_percentage * config.head_size))\n",
    "        self.rope_cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "        self.mask_cache: Optional[Tensor] = None\n",
    "        self.kv_caches: List[Tuple[Tensor, Tensor]] = []\n",
    "\n",
    "    def get_kv_head_dim(self, config: Config) -> int:\n",
    "        queries_per_kv = config.n_head // config.n_query_groups\n",
    "        key_value_heads = config.n_head // queries_per_kv\n",
    "        kv_dim = config.hidden_dim // config.n_head * key_value_heads\n",
    "        return kv_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        max_length: int = None,\n",
    "        use_kv_cache: bool = False,\n",
    "        input_pos: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tensor:\n",
    "      \n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        assert T <= self.config.seq_length, f\"Input length {T} exceeds maximum model length {self.config.seq_length}\"\n",
    "        \n",
    "        max_seq_length = self.config.seq_length\n",
    "        x = self.embed_tokens(input_ids)  \n",
    "        \n",
    "        if self.rope_cache is None :\n",
    "            self.rope_cache = self.build_rope_cache(x)\n",
    "            \n",
    "        if use_kv_cache and self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(input_ids)\n",
    "        \n",
    "        cos, sin = self.rope_cache\n",
    "        \n",
    "        if use_kv_cache:\n",
    "              cos = cos.index_select(0, input_pos)\n",
    "              sin = sin.index_select(0, input_pos)\n",
    "              mask = self.mask_cache.index_select(2, input_pos)\n",
    "              mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            cos = cos[:T]\n",
    "            sin = sin[:T]\n",
    "            mask = None\n",
    "\n",
    "        \n",
    "        if not use_kv_cache:\n",
    "            for block in self.layers:\n",
    "                x = block(x, (cos, sin), max_seq_length)\n",
    "        else:\n",
    "            self.kv_caches = self.kv_caches or self.build_kv_caches(x, max_seq_length, cos.size(-1) * 2)\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, (cos, sin), max_seq_length, mask, input_pos, self.kv_caches[i])\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return self.lm_head(x) \n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        position_ids = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
    "        return self.rope(idx, position_ids)\n",
    "        \n",
    "    def build_mask_cache(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        ones = torch.ones((self.config.seq_length, self.config.seq_length), device=idx.device, dtype=torch.bool)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def build_kv_caches(self, idx: torch.Tensor, max_seq_length: int, rope_cache_length: int) :\n",
    "        B = idx.size(0)\n",
    "        heads = 1 if self.config.n_query_groups == 1 else self.config.n_query_groups\n",
    "\n",
    "        k_cache_shape = (\n",
    "            B,\n",
    "            max_seq_length,\n",
    "            heads,\n",
    "            rope_cache_length + self.config.head_size - int(self.config.rotary_percentage * self.config.head_size),\n",
    "        )\n",
    "        v_cache_shape = (B, max_seq_length, heads, self.config.head_size)\n",
    "        device = idx.device\n",
    "        return [\n",
    "            (torch.zeros(k_cache_shape, device=device), torch.zeros(v_cache_shape, device=device))\n",
    "            for _ in range(self.config.n_layer)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from collections import OrderedDict\n",
    "\n",
    "def remap_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Remaps the state dict keys from LlamaForCausalLM format to your custom model format\n",
    "    \"\"\"\n",
    "    new_state_dict = OrderedDict()\n",
    "    \n",
    "    # Create a mapping dictionary for the different naming conventions\n",
    "    key_mapping = {\n",
    "        'model.embed_tokens': 'embed_tokens',\n",
    "        'model.norm': 'norm',\n",
    "        'model.layers': 'layers',\n",
    "        'self_attn': 'attn',\n",
    "        'input_layernorm': 'norm1',\n",
    "        'post_attention_layernorm': 'norm2',\n",
    "        'mlp': 'ffn'\n",
    "    }\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key\n",
    "        \n",
    "        # Apply the mappings\n",
    "        for old, new in key_mapping.items():\n",
    "            new_key = new_key.replace(old, new)\n",
    "            \n",
    "        # Handle specific cases where tensor shapes need to be validated\n",
    "        if 'attn' in new_key:\n",
    "            # Ensure attention weights have compatible shapes\n",
    "            if value.shape != state_dict[key].shape:\n",
    "                raise ValueError(f\"Incompatible shape for attention weights: {key}\")\n",
    "                \n",
    "        new_state_dict[new_key] = value\n",
    "    \n",
    "    return new_state_dict\n",
    "\n",
    "def load_model_weights(model, safetensors_path):\n",
    "    \"\"\"\n",
    "    Loads and remaps weights from a safetensors file to your custom model\n",
    "    \n",
    "    Args:\n",
    "        model: Your custom model instance\n",
    "        safetensors_path: Path to the safetensors file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the safetensors file\n",
    "        original_state_dict = load_file(safetensors_path)\n",
    "        \n",
    "        # Remap the state dict to match your model's architecture\n",
    "        new_state_dict = remap_state_dict(original_state_dict)\n",
    "        \n",
    "        # Load the remapped weights into your model\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        if missing_keys:\n",
    "            print(\"Missing keys:\", missing_keys)\n",
    "        if unexpected_keys:\n",
    "            print(\"Unexpected keys:\", unexpected_keys)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    }
   ],
   "source": [
    "tiny_LLaMA_1b = LlamaModel(name_to_config[\"tiny_LLaMA_1b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_weights(tiny_LLaMA_1b, \"model/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,  map_device=\"auto\", add_eos_token=True, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"model\",\n",
    "    # attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out1 = tiny_LLaMA_1b(inputs_ids)\n",
    "out2 = model(inputs_ids, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (out1 == out2.logits).all(), \"The model output is different from the reference model output\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
